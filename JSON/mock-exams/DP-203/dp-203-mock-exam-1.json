[
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have a table in an Azure Synapse Analytics dedicated SQL pool. The table was created by using the following Transact-SQL statement.   You need to alter the table to meet the following requirements: \n\n  • Ensure that users can identify the current manager of employees.  \n  • Support creating an employee reporting hierarchy for your entire company. \n  •  Provide fast lookup of the managers' attributes such as name and job title.  \n\n  Which column should you add to the table?",
    "options": [
      "[ManagerEmployeeID] [smallint] NULL.",
      "[ManagerEmployeeKey] [smallint] NULL.",
      "[ManagerEmployeeKey] [int] NULL.",
      "[ManagerName] [varchar](200) NULL."
    ],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q1.1.png",
    "correctAnswer": ["[ManagerEmployeeKey] [int] NULL."],
    "type": "single",
    "explanation": "We need an extra column to identify the Manager. Use the data type as the EmployeeKey column, an int column. Reference: https://docs.microsoft.com/en-us/analysis-services/tabular-models/hierarchies-ssas-tabular"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have an Azure Synapse workspace named MyWorkspace that contains an Apache Spark database named mytestdb. \n\n  You run the following command in an Azure Synapse Analytics Spark pool in MyWorkspace. \n\n  CREATE TABLE mytestdb.myParquetTable(  \n  EmployeeID int, \n   EmployeeName string,   EmployeeStartDate date)  \n\n  USING Parquet -  \n You then use Spark to insert a row into mytestdb.myParquetTable. The row contains the following data.\n\n   One minute later, you execute the following query from a serverless SQL pool in MyWorkspace. \n\n  SELECT EmployeeID -  \n FROM mytestdb.dbo.myParquetTable   WHERE EmployeeName = 'Alice';\n\n   What will be returned by the query?",
    "options": ["24.", "An error.", "A null value ."],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q2.1.png",
    "correctAnswer": ["24."],
    "type": "single",
    "explanation": "Once a database has been created by a Spark job, you can create tables in it with Spark that use Parquet as the storage format. Table names will be converted to lower case and need to be queried using the lower case name. These tables will immediately become available for querying by any of the Azure Synapse workspace Spark pools. They can also be used from any of the Spark jobs subject to permissions. Note: For external tables, since they are synchronized to serverless SQL pool asynchronously, there will be a delay until they appear.   Reference: https://docs.microsoft.com/en-us/azure/synapse-analytics/metadata/table"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have a table named SalesFact in an enterprise data warehouse in Azure Synapse Analytics. SalesFact contains sales data from the past 36 months and has the following characteristics: \n\n • Is partitioned by month \n •  Contains one billion rows \n •  Has clustered columnstore index \n\n  At the beginning of each month, you need to remove data from SalesFact that is older than 36 months as quickly as possible. \n\n  Which three actions should you perform in sequence in a stored procedure? \n\n  Arrange the steps in the correct order.",
    "options": [
      "Switch the partition containing the stale data from SalesFact to SalesFact_Work.",
      "Truncate the partition containing the stale data.",
      "Drop the SalesFact_Work table.",
      "Create an empty table named SalesFact_Work that has the same schema as SalesFact.",
      "Execute a DELETE statement where the value in the Date column is more than 36 months ago.",
      "Copy the data to a new table by using CREATE TABLE AS SELECT (CTAS)."
    ],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q3.1.png",
    "correctAnswer": [
      "Step 1: Create an empty table named SalesFact_Work that has the same schema as SalesFact.",
      "Step 2: Switch the partition containing the stale data from SalesFact to SalesFact_Work.",
      "Step 3: Drop the SalesFact_Work table."
    ],
    "type": "multiple",
    "explanation": "Reference: https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-tables-partition"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have files and folders in Azure Data Lake Storage Gen2 for an Azure Synapse workspace as shown in the following exhibit. \n\n You create an external table named ExtTable that has LOCATION='/topfolder/'. \n\n  When you query ExtTable by using an Azure Synapse Analytics serverless SQL pool, which files are returned?",
    "options": [
      "File2.csv and File3.csv only.",
      "File1.csv and File4.csv only.",
      "File1.csv, File2.csv, File3.csv, and File4.csv.",
      "File1.csv only ."
    ],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q4.1.png",
    "correctAnswer": ["File1.csv and File4.csv only."],
    "type": "single",
    "explanation": "https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&preserve-view=true&tabs=dedicated#location--folder_or_filepath-1"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You are planning the deployment of Azure Data Lake Storage Gen2. You have the following report that will access the data lake: \n • Report1: Reads three columns from a file that contains 50 columns. \n • You need to recommend in which format to store the data in the data lake to support the reports. \n • The solution must minimize read times. \n\n  What should you recommend for the report? ",
    "options": ["Avro.", "CSV.", "Parquet.", "TSV."],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q5.1.png",
    "correctAnswer": ["CSV."],
    "type": "single",
    "explanation": "The destination writes records as delimited data. Reference: https://streamsets.com/documentation/datacollector/latest/help/datacollector/UserGuide/Destinations/ADLS-G2-D.html"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You are planning the deployment of Azure Data Lake Storage Gen2. You have the following report that will access the data lake: \n\n  Report2: Queries a single record based on a timestamp. \n\n  You need to recommend in which format to store the data in the data lake to support the reports. \n\n The solution must minimize read times. \n\n  What should you recommend for the report? ",
    "options": ["Avro.", "CSV.", "Parquet.", "TSV."],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q5.1.png",
    "correctAnswer": ["Avro."],
    "type": "single",
    "explanation": "AVRO supports timestamps. Reference: https://streamsets.com/documentation/datacollector/latest/help/datacollector/UserGuide/Destinations/ADLS-G2-D.html"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You are designing the folder structure for an Azure Data Lake Storage Gen2 container. \n\n  Users will query data by using a variety of services including Azure Databricks and Azure Synapse Analytics serverless SQL pools. The data will be secured by subject area. Most queries will include data from the current year or current month. \n\n  Which folder structure should you recommend to support fast queries and simplified folder security?",
    "options": [
      "/{SubjectArea}/{DataSource}/{DD}/{MM}/{YYYY}/{FileData}_{YYYY}_{MM}_{DD}.csv",
      "/{DD}/{MM}/{YYYY}/{SubjectArea}/{DataSource}/{FileData}_{YYYY}_{MM}_{DD}.csv",
      "/{YYYY}/{MM}/{DD}/{SubjectArea}/{DataSource}/{FileData}_{YYYY}_{MM}_{DD}.csv",
      "/{SubjectArea}/{DataSource}/{YYYY}/{MM}/{DD}/{FileData}_{YYYY}_{MM}_{DD}.csv"
    ],
    "correctAnswer": [
      "/{SubjectArea}/{DataSource}/{YYYY}/{MM}/{DD}/{FileData}_{YYYY}_{MM}_{DD}.csv"
    ],
    "type": "single",
    "explanation": "There's an important reason to put the date at the end of the directory structure. If you want to lock down certain regions or subject matters to users/groups, then you can easily do so with the POSIX permissions. Otherwise, if there was a need to restrict a certain security group to viewing just the UK data or certain planes, with the date structure in front a separate permission would be required for numerous directories under every hour directory. Additionally, having the date structure in front would exponentially increase the number of directories as time went on.   Note: In IoT workloads, there can be a great deal of data being landed in the data store that spans across numerous products, devices, organizations, and customers. It's important to pre-plan the directory layout for organization, security, and efficient processing of the data for down-stream consumers. A general template to consider might be the following layout: {Region}/{SubjectMatter(s)}/{yyyy}/{mm}/{dd}/{hh}/"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You need to output files from Azure Data Factory. Which file format should you use for 'Columnar format' output?",
    "options": ["Avro.", "GZip.", "Parquet.", "TXT."],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q8.1.png",
    "correctAnswer": ["Parquet."],
    "type": "single",
    "explanation": "Parquet stores data in columns, while Avro stores data in a row-based format. By their very nature, column-oriented data stores are optimized for read-heavy analytical workloads, while row-based databases are best for write-heavy transactional workloads. Reference: https://www.datanami.com/2018/05/16/big-data-file-formats-demystified"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You need to output files from Azure Data Factory. Which file format should you use for 'JSON with a timestamp' output?",
    "options": ["Avro.", "GZip.", "Parquet.", "TXT."],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q8.1.png",
    "correctAnswer": ["Avro."],
    "type": "single",
    "explanation": "An Avro schema is created using JSON format. AVRO supports timestamps. Note: Azure Data Factory supports the following file formats (not GZip or TXT).   Reference: https://www.datanami.com/2018/05/16/big-data-file-formats-demystified"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You use Azure Data Factory to prepare data to be queried by Azure Synapse Analytics serverless SQL pools. Files are initially ingested into an Azure Data Lake Storage Gen2 account as 10 small JSON files. Each file contains the same data attributes and data from a subsidiary of your company. You need to move the files to a different folder and transform the data to meet the following requirements: \n\n• Provide the fastest possible query times.  \n• Automatically infer the schema from the underlying files. \n\n How should you configure the Data Factory copy activity?",
    "options": ["Flatten Hierarcy.", "Merge files.", "Preserve Hierarcy."],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q10.1.png",
    "correctAnswer": ["Preserve Hierarcy."],
    "type": "single",
    "explanation": "Compared to the flat namespace on Blob storage, the hierarchical namespace greatly improves the performance of directory management operations, which improves overall job performance."
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You use Azure Data Factory to prepare data to be queried by Azure Synapse Analytics serverless SQL pools. Files are initially ingested into an Azure Data Lake Storage Gen2 account as 10 small JSON files. Each file contains the same data attributes and data from a subsidiary of your company. You need to move the files to a different folder and transform the data to meet the following requirements:  \n• Provide the fastest possible query times.  \n• Automatically infer the schema from the underlying files. \n\n  How should you configure the Data Factory copy activity?",
    "options": ["CSV.", "Json.", "Parquet.", "TXT."],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q11.1.png",
    "correctAnswer": ["Parquet."],
    "type": "single",
    "explanation": "Azure Data Factory parquet format is supported for Azure Data Lake Storage Gen2. Parquet supports the schema property.   Reference: https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction   https://docs.microsoft.com/en-us/azure/data-factory/format-parquet"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have an Azure Data Lake Storage Gen2 container. Data is ingested into the container, and then transformed by a data integration application. The data is NOT modified after that. Users can read files in the container but cannot modify the files. \n\n  You need to design a data archiving solution that meets the following requirements: \n\n•  New data is accessed frequently and must be available as quickly as possible. \n•  Data that is older than five years is accessed infrequently but must be available within one second when requested. \n•  Costs must be minimized while maintaining the required availability. \n\n  How should you manage for 'Five-year old data'?",
    "options": [
      "Delete the blob.",
      "Move to archive storage.",
      "Move to cool storage.",
      "Move to hot storage."
    ],
    "correctAnswer": ["Move to cool storage."],
    "type": "single",
    "explanation": "Reference: https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers "
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have an Azure Data Lake Storage Gen2 container. Data is ingested into the container, and then transformed by a data integration application. The data is NOT modified after that. Users can read files in the container but cannot modify the files.  \n\n You need to design a data archiving solution that meets the following requirements:  \n\n• Data that is older than seven years is NOT accessed. \n• After seven years, the data must be persisted at the lowest cost possible.  \n\n How should you manage for 'Seven-year old data'?",
    "options": [
      "Delete the blob.",
      "Move to archive storage.",
      "Move to cool storage.",
      "Move to hot storage."
    ],
    "correctAnswer": ["Move to archive storage."],
    "type": "single",
    "explanation": "Optimized for storing data that is rarely accessed and stored for at least 180 days with flexible latency requirements, on the order of hours.   Reference: https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers "
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have a data model that you plan to implement in a data warehouse in Azure Synapse Analytics as shown in the following exhibit. All the dimension tables will be less than 2 GB after compression, and the fact table will be approximately 6 TB. The dimension tables will be relatively static with very few data inserts and updates. \n\n  Which type of table should you use for 'Dim_Employee'?",
    "options": ["Hash distributed.", "Round-robin.", "Replicated."],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q14.1.png",
    "correctAnswer": ["Replicated."],
    "type": "single",
    "explanation": ""
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have a data model that you plan to implement in a data warehouse in Azure Synapse Analytics as shown in the following exhibit. All the dimension tables will be less than 2 GB after compression, and the fact table will be approximately 6 TB. The dimension tables will be relatively static with very few data inserts and updates. \n\n  Which type of table should you use for 'Dim_Time'?",
    "options": ["Hash distributed.", "Round-robin.", "Replicated."],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q14.1.png",
    "correctAnswer": ["Replicated."],
    "type": "single",
    "explanation": ""
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have a data model that you plan to implement in a data warehouse in Azure Synapse Analytics as shown in the following exhibit.   All the dimension tables will be less than 2 GB after compression, and the fact table will be approximately 6 TB. The dimension tables will be relatively static with very few data inserts and updates. \n\n  Which type of table should you use for 'Fact_DailyBookings'?",
    "options": ["Hash distributed.", "Round-robin.", "Replicated."],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q14.1.png",
    "correctAnswer": ["Hash distributed."],
    "type": "single",
    "explanation": "For Fact tables use hash-distribution with clustered columnstore index. Performance improves when two hash tables are joined on the same distribution column.   Reference: https://azure.microsoft.com/en-us/updates/reduce-data-movement-and-make-your-queries-more-efficient-with-the-general-availability-of-replicated-tables/ https://azure.microsoft.com/en-us/blog/replicated-tables-now-generally-available-in-azure-sql-data-warehouse/"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have a data model that you plan to implement in a data warehouse in Azure Synapse Analytics as shown in the following exhibit.   All the dimension tables will be less than 2 GB after compression, and the fact table will be approximately 6 TB. The dimension tables will be relatively static with very few data inserts and updates. \n\n  Which type of table should you use for 'Dim_Customer'?",
    "options": ["Hash distributed.", "Round-robin.", "Replicated."],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q14.1.png",
    "correctAnswer": ["Replicated."],
    "type": "single",
    "explanation": "Replicated tables are ideal for small star-schema dimension tables, because the fact table is often distributed on a column that is not compatible with the connected dimension tables. If this case applies to your schema, consider changing small dimension tables currently implemented as round-robin to replicated."
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You need to create a partitioned table in an Azure Synapse Analytics dedicated SQL pool. How should you complete the Transact-SQL statement?   Fill in the boexs.",
    "options": [
      "CLUSTERED INDEX.",
      "COLLATE.",
      "DISTRIBUTION.",
      "PARTITION.",
      "PARTITION FUNCTION.",
      "PARTITION SCHEME."
    ],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q18.1.png",
    "correctAnswer": ["DISTRIBUTION.", "PARTITION."],
    "type": "multiple",
    "explanation": "Box 1: DISTRIBUTION - Table distribution options include DISTRIBUTION = HASH ( distribution_column_name ), assigns each row to one distribution by hashing the value stored in distribution_column_name.   Box 2: PARTITION - Table partition options. Syntax: PARTITION ( partition_column_name RANGE [ LEFT | RIGHT ] FOR VALUES ( [ boundary_value [,...n] ] ))   Reference: https://docs.microsoft.com/en-us/sql/t-sql/statements/create-table-azure-sql-data-warehouse?"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You need to design an Azure Synapse Analytics dedicated SQL pool that meets the following requirements: \n\n  Can return an employee record from a given point in time. \n•  Maintains the latest employee information. \n•  Minimizes query complexity. \n\n  How should you model the employee data?",
    "options": [
      "As a temporal table.",
      "As a SQL graph table.",
      "As a degenerate dimension table.",
      "As a Type 2 slowly changing dimension (SCD) table."
    ],
    "correctAnswer": ["As a Type 2 slowly changing dimension (SCD) table."],
    "type": "single",
    "explanation": "A Type 2 SCD supports versioning of dimension members. Often the source system doesn't store versions, so the data warehouse load process detects and manages changes in a dimension table. In this case, the dimension table must use a surrogate key to provide a unique reference to a version of the dimension member. It also includes columns that define the date range validity of the version (for example, StartDate and EndDate) and possibly a flag column (for example, IsCurrent) to easily filter by current dimension members.   Reference: https://docs.microsoft.com/en-us/learn/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have an enterprise-wide Azure Data Lake Storage Gen2 account. The data lake is accessible only through an Azure virtual network named VNET1.   You are building a SQL pool in Azure Synapse that will use data from the data lake.   Your company has a sales team. All the members of the sales team are in an Azure Active Directory group named Sales. POSIX controls are used to assign the Sales group access to the files in the data lake.  You plan to load data to the SQL pool every hour.   You need to ensure that the SQL pool can load the sales data from the data lake. \n\n  Which three actions should you perform?",
    "options": [
      "Add the managed identity to the Sales group.",
      "Use the managed identity as the credentials for the data load process.",
      "Create a shared access signature (SAS).",
      "Add your Azure Active Directory (Azure AD) account to the Sales group.",
      "Use the shared access signature (SAS) as the credentials for the data load process.",
      "Create a managed identity."
    ],
    "correctAnswer": [
      "Add the managed identity to the Sales group.",
      "Use the managed identity as the credentials for the data load process.",
      "Create a managed identity."
    ],
    "type": "multiple",
    "explanation": "The managed identity grants permissions to the dedicated SQL pools in the workspace.   Note: Managed identity for Azure resources is a feature of Azure Active Directory. The feature provides Azure services with an automatically managed identity in Azure AD.   Reference: https://docs.microsoft.com/en-us/azure/synapse-analytics/security/synapse-workspace-managed-identity"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have an Azure Synapse Analytics dedicated SQL pool that contains the users shown in the following table. User1 is the only user who has access to the unmasked data. \n\n  Complete the following statement based on the information presented in the graphic:\n\n 'When User2 queries the YearlyIncome column, the values returned will be ______.'",
    "options": [
      "A random number.",
      "The values stored in the database.",
      "XXXX.",
      "0."
    ],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q21.1.png",
    "correctAnswer": ["0."],
    "type": "single",
    "explanation": "The YearlyIncome column is of the money data type. The Default masking function: Full masking according to the data types of the designated fields. Use a zero value for numeric data types (bigint, bit, decimal, int, money, numeric, smallint, smallmoney, tinyint, float, real)."
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have an Azure Synapse Analytics dedicated SQL pool that contains the users shown in the following table. User1 is the only user who has access to the unmasked data. \n\n  Complete the following statement based on the information presented in the graphic: \n\n'When User1 queries the BirthDate column, the values returned will be _______.'",
    "options": [
      "A random date.",
      "The values stored in the database.",
      "XXXX.",
      "1900-01-01."
    ],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q21.1.png",
    "correctAnswer": ["The values stored in the database."],
    "type": "single",
    "explanation": "Users with administrator privileges are always excluded from masking, and see the original data without any mask.   Reference: https://docs.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have an enterprise data warehouse in Azure Synapse Analytics.   Using PolyBase, you create an external table named [Ext].[Items] to query Parquet files stored in Azure Data Lake Storage Gen2 without importing the data to the data warehouse. The external table has three columns.   You discover that the Parquet files have a fourth column named ItemID. \n\n  Which command should you run to add the ItemID column to the external table?",
    "options": [
      "ALTER EXTERNAL TABLE (Ext).   [Items] ADD [ItemID] int;.",
      "DROP EXTERNAL FILE FORMAT parquetfile1;   CREATE EXTERNAL FILE FORMAT parquetfilel   WITH (   FORMAT TYPE = PARQUET,   DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'  );",
      "DROP EXTERNAL TABLE (Ext). [Items]   CREATE EXTERNAL TABLE (Ext). [Items]   ([ItemID] (int) NULL,   [ItemName] nvarchar(50) NULL,   [ItemType) nvarchar(20) NULL,   [ItemDescription) nvarchar(250))   WITH   (   LOCATION= '/Items/',   DATA_SOURCE = AzureDataLakeStore,   FILE FORMAT PARQUET,   REJECT TYPE = VALUE,   REJECT_VALUE = 0   );",
      "ALTER TABLE (Ext). [Items]   ADD [ItemID) int;."
    ],
    "correctAnswer": [
      "DROP EXTERNAL TABLE (Ext). [Items]   CREATE EXTERNAL TABLE (Ext). [Items]   ([ItemID] (int) NULL,   [ItemName] nvarchar(50) NULL,   [ItemType) nvarchar(20) NULL,   [ItemDescription) nvarchar(250))   WITH   (   LOCATION= '/Items/',   DATA_SOURCE = AzureDataLakeStore,   FILE FORMAT PARQUET,   REJECT TYPE = VALUE,   REJECT_VALUE = 0   );"
    ],
    "type": "single",
    "explanation": "Reference: https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql "
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have two Azure Storage accounts named Storage1 and Storage2. Each account holds one container and has the hierarchical namespace enabled. The system has files that contain data stored in the Apache Parquet format. \n\n  You need to copy folders and files from Storage1 to Storage2 by using a Data Factory copy activity. The solution must meet the following requirements: \n\n•  No transformations must be performed.  \n• The original folder structure must be retained. \n•  Minimize time required to perform the copy activity. \n\n  How should you configure the copy activity for 'Source Dataset Type'?",
    "options": ["Binary.", "Parquet.", "Delimited text."],
    "correctAnswer": ["Parquet."],
    "type": "single",
    "explanation": "For Parquet datasets, the type property of the copy activity source must be set to ParquetSource.   Reference: https://docs.microsoft.com/en-us/azure/data-factory/format-parquet"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have two Azure Storage accounts named Storage1 and Storage2. Each account holds one container and has the hierarchical namespace enabled. The system has files that contain data stored in the Apache Parquet format.  \n\n You need to copy folders and files from Storage1 to Storage2 by using a Data Factory copy activity. The solution must meet the following requirements: \n\n•  No transformations must be performed. \n•  The original folder structure must be retained. \n•  Minimize time required to perform the copy activity. \n\n  How should you configure the copy activity for 'Copy activity copy behaviour'?",
    "options": ["Flatten Hierarcy.", "Merge Files.", "Preserve Hierarcy."],
    "correctAnswer": ["Preserve Hierarcy."],
    "type": "single",
    "explanation": "Preserves the file hierarchy in the target folder. The relative path of the source file to the source folder is identical to the relative path of the target file to the target folder.   Reference: https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have an Azure Data Lake Storage Gen2 container that contains 100 TB of data.  You need to ensure that the data in the container is available for read workloads in a secondary region if an outage occurs in the primary region. \n\n The solution must minimize costs. \n\n  Which type of data redundancy should you use?",
    "options": [
      "Geo-redundant storage (GRS).",
      "Read-access geo-redundant storage (RA-GRS).",
      "Zone-redundant storage (ZRS).",
      "Locally-redundant storage (LRS)."
    ],
    "correctAnswer": ["Read-access geo-redundant storage (RA-GRS)."],
    "type": "single",
    "explanation": "Read-access geo-redundant storage replicates your data to another physical location in the secondary region to protect against regional outages. However, that data is available to be read only if the customer or Microsoft initiates a failover from the primary to secondary region. When you enable read access to the secondary region, your data is available to be read at all times, including in a situation where the primary region becomes unavailable.   Reference: https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You plan to implement an Azure Data Lake Gen 2 storage account.   You need to ensure that the data lake will remain available if a data center fails in the primary Azure region. The solution must minimize costs. \n\n  Which type of replication should you use for the storage account?",
    "options": [
      "Geo-redundant storage (GRS).",
      "Geo-zone-redundant storage (GZRS).",
      "Locally-redundant storage (LRS).",
      "Zone-redundant storage (ZRS)."
    ],
    "correctAnswer": ["Zone-redundant storage (ZRS)."],
    "type": "single",
    "explanation": "Zone-redundant storage (ZRS) copies your data synchronously across three Azure availability zones in the primary region.   Reference: https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have a SQL pool in Azure Synapse. You plan to load data from Azure Blob storage to a staging table. Approximately 1 million rows of data will be loaded daily. The table will be truncated before each daily load.   You need to create the staging table. The solution must minimize how long it takes to load the data to the staging table. \n\n  How should you configure the table for 'Distribution'?",
    "options": ["Hash.", "Replicated.", "Round-robin."],
    "correctAnswer": ["Hash."],
    "type": "single",
    "explanation": "Hash-distributed tables improve query performance on large fact tables. They can have very large numbers of rows and still achieve high performance.   Reference: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have a SQL pool in Azure Synapse. You plan to load data from Azure Blob storage to a staging table. Approximately 1 million rows of data will be loaded daily. The table will be truncated before each daily load.   You need to create the staging table. The solution must minimize how long it takes to load the data to the staging table. \n\n  How should you configure the table for 'Indexing'?",
    "options": ["Clustered.", "Clustered columnstore.", "Heap."],
    "correctAnswer": ["Clustered columnstore."],
    "type": "single",
    "explanation": "When creating partitions on clustered columnstore tables, it is important to consider how many rows belong to each partition. For optimal compression and performance of clustered columnstore tables, a minimum of 1 million rows per distribution and partition is needed."
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have a SQL pool in Azure Synapse. You plan to load data from Azure Blob storage to a staging table. Approximately 1 million rows of data will be loaded daily. The table will be truncated before each daily load.   You need to create the staging table. The solution must minimize how long it takes to load the data to the staging table.  \n\n How should you configure the table for 'Partitioning'?",
    "options": ["Date.", "None."],
    "correctAnswer": ["Date."],
    "type": "single",
    "explanation": "Table partitions enable you to divide your data into smaller groups of data. In most cases, table partitions are created on a date column. Partition switching can be used to quickly remove or replace a section of a table.   Reference: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You are designing a fact table named FactPurchase in an Azure Synapse Analytics dedicated SQL pool. The table contains purchases from suppliers for a retail store. FactPurchase will contain the following columns. \n\n FactPurchase will have 1 million rows of data added daily and will contain three years of data. \n  Transact-SQL queries similar to the following query will be executed daily. \n\n  SELECT - \n  SupplierKey, StockItemKey, IsOrderFinalized, COUNT(*)\n   FROM FactPurchase - \n\n  WHERE DateKey >= 20210101 - \n\n  AND DateKey <= 20210131 -  \n GROUP By SupplierKey, StockItemKey, IsOrderFinalized \n\n  Which table distribution will minimize query times?",
    "options": [
      "Replicated.",
      "Hash-distributed on PurchaseKey.",
      "Round-robin.",
      "Hash-distributed on IsOrderFinalized ."
    ],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/31.1.png",
    "correctAnswer": ["Hash-distributed on PurchaseKey."],
    "type": "single",
    "explanation": "Hash-distributed tables improve query performance on large fact tables.   To balance the parallel processing, select a distribution column that:   Has many unique values. The column can have duplicate values. All rows with the same value are assigned to the same distribution. Since there are 60 distributions, some distributions can have > 1 unique values while others may end with zero values.   Does not have NULLs, or has only a few NULLs.   Is not a date column.   Reference: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "From a website analytics system, you receive data extracts about user interactions such as downloads, link clicks, form submissions, and video plays.   The data contains the following columns. \n\n  You need to design a star schema to support analytical queries of the data. The star schema will contain four tables including a date dimension. \n\n  To which table should you add 'EventCategory:'?",
    "options": ["DimChannel.", "DimDate.", "DimEvent.", "FactEvents."],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q32.1.png",
    "correctAnswer": ["DimEvent."],
    "type": "single",
    "explanation": ""
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "From a website analytics system, you receive data extracts about user interactions such as downloads, link clicks, form submissions, and video plays.   The data contains the following columns. \n\n  You need to design a star schema to support analytical queries of the data. The star schema will contain four tables including a date dimension.  \n\n To which table should you add 'ChannelGrouping:'?",
    "options": ["DimChannel.", "DimDate.", "DimEvent.", "FactEvents."],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q32.1.png",
    "correctAnswer": ["DimChannel."],
    "type": "single",
    "explanation": ""
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "From a website analytics system, you receive data extracts about user interactions such as downloads, link clicks, form submissions, and video plays.   The data contains the following columns. \n\n  You need to design a star schema to support analytical queries of the data. The star schema will contain four tables including a date dimension. \n\n  To which table should you add 'TotalEvents:'?",
    "options": ["DimChannel.", "DimDate.", "DimEvent.", "FactEvents."],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q32.1.png",
    "correctAnswer": ["FactEvents."],
    "type": "single",
    "explanation": "Fact tables store observations or events, and can be sales orders, stock balances, exchange rates, temperatures, etc.   Reference: https://docs.microsoft.com/en-us/power-bi/guidance/star-schema"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB. \n\n  You plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.   You need to prepare the files to ensure that the data copies quickly. \n\n  Solution: 'You convert the files to compressed delimited text files.'   Does this meet the goal?",
    "options": ["Yes.", "No."],
    "correctAnswer": ["No."],
    "type": "single",
    "explanation": "All file formats have different performance characteristics. For the fastest load, use compressed delimited text files.   Reference: https://docs.microsoft.com/en-us/azure/sql-data-warehouse/guidance-for-loading-data"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB. \n\n  You plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.   You need to prepare the files to ensure that the data copies quickly. \n\n  Solution: 'You modify the files to ensure that each row is more than 1 MB.'   Does this meet the goal?",
    "options": ["Yes.", "No."],
    "correctAnswer": ["No."],
    "type": "single",
    "explanation": "Instead convert the files to compressed delimited text files.   Reference: https://docs.microsoft.com/en-us/azure/sql-data-warehouse/guidance-for-loading-data"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB. \n\n  You plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.   You need to prepare the files to ensure that the data copies quickly. \n\n  Solution: 'You copy the files to a table that has a columnstore index.'   Does this meet the goal?",
    "options": ["Yes.", "No."],
    "correctAnswer": ["No."],
    "type": "single",
    "explanation": "Instead convert the files to compressed delimited text files.   Reference: https://docs.microsoft.com/en-us/azure/sql-data-warehouse/guidance-for-loading-data"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You build a data warehouse in an Azure Synapse Analytics dedicated SQL pool. Analysts write a complex SELECT query that contains multiple JOIN and CASE statements to transform data for use in inventory reports. The inventory reports will use the data and additional WHERE parameters depending on the report. The reports will be produced once daily. \n\n  You need to implement a solution to make the dataset available for the reports. The solution must minimize query times. \n\n  What should you implement?",
    "options": [
      "An ordered clustered columnstore index.",
      "A materialized view.",
      "Result set caching.",
      "A replicated table ."
    ],
    "correctAnswer": ["A materialized view."],
    "type": "single",
    "explanation": "Materialized views for dedicated SQL pools in Azure Synapse provide a low maintenance method for complex analytical queries to get fast performance without any query change.   Reference: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/performance-tuning-materialized-views"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have an Azure Synapse Analytics workspace named WS1 that contains an Apache Spark pool named Pool1. You plan to create a database named DB1 in Pool1.  \n\n You need to ensure that when tables are created in DB1, the tables are available automatically as external tables to the built-in serverless SQL pool. \n\n   Which format should you use for the tables in DB1?",
    "options": ["CSV.", "ORC.", "JSON.", "Parquet."],
    "correctAnswer": ["Parquet."],
    "type": "single",
    "explanation": "Serverless SQL pool can automatically synchronize metadata from Apache Spark. A serverless SQL pool database will be created for each database existing in serverless Apache Spark pools.   For each Spark external table based on Parquet or CSV and located in Azure Storage, an external table is created in a serverless SQL pool database.   Reference: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-storage-files-spark-tables"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You are planning a solution to aggregate streaming data that originates in Apache Kafka and is output to Azure Data Lake Storage Gen2. The developers who will implement the stream processing solution use Java. \n\n  Which service should you recommend using to process the streaming data?",
    "options": [
      "Azure Event Hubs.",
      "Azure Data Factory.",
      "Azure Stream Analytics.",
      "Azure Databricks."
    ],
    "correctAnswer": ["Azure Databricks."],
    "type": "single",
    "explanation": "Reference: https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You plan to implement an Azure Data Lake Storage Gen2 container that will contain CSV files. The size of the files will vary based on the number of events that occur per hour. File sizes range from 4 KB to 5 GB. \n\n  You need to ensure that the files stored in the container are optimized for batch processing. \n\n  What should you do?",
    "options": [
      "Convert the files to JSON.",
      "Convert the files to Avro.",
      "Compress the files.",
      "Merge the files."
    ],
    "correctAnswer": ["Convert the files to Avro."],
    "type": "single",
    "explanation": "Avro supports batch and is very relevant for streaming.   Note: Avro is framework developed within Apache's Hadoop project. It is a row-based storage format which is widely used as a serialization process. AVRO stores its schema in JSON format making it easy to read and interpret by any program. The data itself is stored in binary format by doing it compact and efficient.   Reference: https://www.adaltas.com/en/2020/07/23/benchmark-study-of-different-file-format/"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You store files in an Azure Data Lake Storage Gen2 container. The container has the storage policy shown in the following exhibit.  \n\n The files are ________ after 30 days:",
    "options": [
      "Deleted from the container.",
      "Moved to archive storage.",
      "Moved to cool storage.",
      "Moved to hot storage."
    ],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q42.1.png",
    "correctAnswer": ["Moved to cool storage."],
    "type": "single",
    "explanation": "The ManagementPolicyBaseBlob.TierToCool property gets or sets the function to tier blobs to cool storage. Support blobs currently at Hot tier.   Reference: https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.management.storage.fluent.models.managementpolicybaseblob.tiertocool"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You store files in an Azure Data Lake Storage Gen2 container. The container has the storage policy shown in the following exhibit. \n\n  The storage policy applies to: ________.",
    "options": [
      "container1/contoso.csv.",
      "container1/docs/contoso.json.",
      "container1/mycontoso/contoso.csv."
    ],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q42.1.png",
    "correctAnswer": ["container1/contoso.csv."],
    "type": "single",
    "explanation": "As defined by prefixMatch, an array of strings for prefixes to be matched. Each rule can define up to 10 case-senstive prefixes. A prefix string must start with a container name."
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You are designing a financial transactions table in an Azure Synapse Analytics dedicated SQL pool. The table will have a clustered columnstore index and will include the following columns: \n\n•  TransactionType: 40 million rows per transaction type \n•  CustomerSegment: 4 million per customer segment   TransactionMonth: 65 million rows per month \n•  AccountType: 500 million per account type \n\n You have the following query requirements: \n\n•Analysts will most commonly analyze transactions for a given month. \n\n  Transactions analysis will typically summarize transactions by transaction type, customer segment, and/or account type   You need to recommend a partition strategy for the table to minimize query times. \n\n  On which column should you recommend partitioning the table?",
    "options": [
      "CustomerSegment.",
      "AccountType.",
      "TransactionType.",
      "TransactionMonth."
    ],
    "correctAnswer": ["TransactionMonth."],
    "type": "single",
    "explanation": "For optimal compression and performance of clustered columnstore tables, a minimum of 1 million rows per distribution and partition is needed. Before partitions are created, dedicated SQL pool already divides each table into 60 distributed databases.   Example: Any partitioning added to a table is in addition to the distributions created behind the scenes. Using this example, if the sales fact table contained 36 monthly partitions, and given that a dedicated SQL pool has 60 distributions, then the sales fact table should contain 60 million rows per month, or 2.1 billion rows when all months are populated. If a table contains fewer than the recommended minimum number of rows per partition, consider using fewer partitions in order to increase the number of rows per partition."
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have an Azure Data Lake Storage Gen2 account named account1 that stores logs as shown in the following table.   You do not expect that the logs will be accessed during the retention periods.   You need to recommend a solution for account1 to 'minimizes storage costs'   What should you include in the recommendation?",
    "options": [
      "Store the infrastructure logs and the application logs in the Archive access tier.",
      "Store the infrastructure logs and the application logs in the Cool access tier.",
      "Store the infrastructure logs in the Cool access tier and the application logs in the Archive access tier."
    ],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q45.1.png",
    "correctAnswer": [
      "Store the infrastructure logs in the Cool access tier and the application logs in the Archive access tier."
    ],
    "type": "single",
    "explanation": "For infrastructure logs: Cool tier - An online tier optimized for storing data that is infrequently accessed or modified. Data in the cool tier should be stored for a minimum of 30 days. The cool tier has lower storage costs and higher access costs compared to the hot tier.    For application logs: Archive tier - An offline tier optimized for storing data that is rarely accessed, and that has flexible latency requirements, on the order of hours. Data in the archive tier should be stored for a minimum of 180 days."
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have an Azure Data Lake Storage Gen2 account named account1 that stores logs as shown in the following table.   You do not expect that the logs will be accessed during the retention periods.   You need to recommend a solution for account1 to 'delete logs automatically:'   What should you include in the recommendation?",
    "options": [
      "Azure Data Factory pipelines.",
      "Azure Blob storage lifecycle management rules.",
      "Immutable Azure Blob storage time-based retention policies."
    ],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q45.1.png",
    "correctAnswer": ["Azure Blob storage lifecycle management rules."],
    "type": "single",
    "explanation": "Blob storage lifecycle management offers a rule-based policy that you can use to transition your data to the desired access tier when your specified conditions are met. You can also use lifecycle management to expire data at the end of its life.   Reference: https://docs.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview "
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You plan to ingest streaming social media data by using Azure Stream Analytics. The data will be stored in files in Azure Data Lake Storage, and then consumed by using Azure Databricks and PolyBase in Azure Synapse Analytics.  \n\n  You need to recommend a Stream Analytics data output format to ensure that the queries from Databricks and PolyBase against the files encounter the fewest possible errors. The solution must ensure that the files can be queried quickly and that the data type information is retained.\n\n What should you recommend?",
    "options": ["JSON.", "Parquet.", "CSV.", "Avro."],
    "correctAnswer": ["Parquet."],
    "type": "single",
    "explanation": "Parquet supports both Databricks and PolyBase. Reference: https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-file-format-transact-sql"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 contains a partitioned fact table named dbo.Sales and a staging table named stg.Sales that has the matching table and partition definitions.   You need to overwrite the content of the first partition in dbo.Sales with the content of the same partition in stg.Sales. The solution must minimize load times.   What should you do?",
    "options": [
      "Insert the data from stg.Sales into dbo.Sales.",
      "Switch the first partition from dbo.Sales to stg.Sales.",
      "Switch the first partition from stg.Sales to dbo.Sales.",
      "Update dbo.Sales from stg.Sales."
    ],
    "correctAnswer": [
      "Switch the first partition from stg.Sales to dbo.Sales."
    ],
    "type": "single",
    "explanation": "Partition switching in Azure Synapse Analytics allows for efficient and rapid replacement of partitioned data between tables, minimizing load times by leveraging metadata operations rather than physically moving data."
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You are designing a slowly changing dimension (SCD) for supplier data in an Azure Synapse Analytics dedicated SQL pool.   You plan to keep a record of changes to the available fields.   The supplier data contains the following columns. \n\n  Which three additional columns should you add to the data to create a Type 2 SCD?",
    "options": [
      "Surrogate primary key.",
      "Effective start date.",
      "Business key.",
      "Last modified date.",
      "Effective end date.",
      "Foreign key."
    ],
    "image": "./assets/images/question-images/dp-203-mock-exam-1/Q49.1.png",
    "correctAnswer": [
      "Surrogate primary key.",
      "Effective start date.",
      "Effective end date."
    ],
    "type": "mutiple",
    "explanation": "A type 2 SCD requires a surrogate key to uniquely identify each record when versioning. Reference: https://docs.microsoft.com/en-us/learn/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types"
  },
  {
    "quizName": "Microsoft Azure Data Engineer (DP-203) Quiz 1",
    "question": "You have a Microsoft SQL Server database that uses a third normal form schema.   You plan to migrate the data in the database to a star schema in an Azure Synapse Analytics dedicated SQL pool.   You need to design the dimension tables. The solution must optimize read operations.   What should you include in the solution for 'transforming data for the dimension tables by:'?",
    "options": [
      "Maintaining to a third normal form.",
      "Normalizing to a fourth normal form.",
      "Denormalizing to a second normal form."
    ],
    "correctAnswer": ["Denormalizing to a second normal form."],
    "type": "single",
    "explanation": "Denormalization is the process of transforming higher normal forms to lower normal forms via storing the join of higher normal form relations as a base relation.   Denormalization increases the performance in data retrieval at cost of bringing update anomalies to a database."
  }
]
